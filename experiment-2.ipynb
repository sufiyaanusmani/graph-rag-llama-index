{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Implementation with LlamaIndex - Experiment 2\n",
    "\n",
    "[GraphRAG - LlamaIndex](https://medium.aiplanet.com/implement-rag-with-knowledge-graph-and-llama-index-6a3370e93cdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index llama-index-graph-stores-neo4j graspologic numpy==1.24.4 scipy==1.12.0 future python-dotenv setuptools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup API Key, LLM, Embed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4\", api_key=Config.OPENAI_API_KEY)\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\", api_key=Config.OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import uuid\n",
    "from ebooklib import epub\n",
    "\n",
    "def extract_epub_metadata(book_path: str) -> dict:\n",
    "    book_path = Path(book_path)\n",
    "    if not book_path.exists():\n",
    "        raise FileNotFoundError(f\"EPUB file not found at path: {book_path}\")\n",
    "    book = epub.read_epub(str(book_path))\n",
    "\n",
    "    return {\n",
    "        \"title\": book.get_metadata(\"DC\", \"title\")[0][0].rstrip(\".epub\") if book.get_metadata(\"DC\", \"title\") else \"N/A\",\n",
    "        \"author\": book.get_metadata(\"DC\", \"creator\")[0][0] if book.get_metadata(\"DC\", \"creator\") else \"\",\n",
    "        \"language\": book.get_metadata(\"DC\", \"language\")[0][0] if book.get_metadata(\"DC\", \"language\") else \"\",\n",
    "        \"description\": book.get_metadata(\"DC\", \"description\")[0][0] if book.get_metadata(\"DC\", \"description\") else \"\",\n",
    "        \"type\": \"epub\",\n",
    "        \"embeddings\": \"openaiembeddings\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Capelin\\graph-rag-llama-index\\graph-rag-llama-index\\.venv\\Lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n",
      "e:\\Capelin\\graph-rag-llama-index\\graph-rag-llama-index\\.venv\\Lib\\site-packages\\ebooklib\\epub.py:1423: FutureWarning: This search incorrectly ignores the root element, and will be fixed in a future version.  If you rely on the current behaviour, change it to './/xmlns:rootfile[@media-type]'\n",
      "  for root_file in tree.findall('//xmlns:rootfile[@media-type]', namespaces={'xmlns': NAMESPACES['CONTAINERNS']}):\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"./data\", file_metadata=extract_epub_metadata).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 documents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the Knowledge Graph Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import KnowledgeGraphIndex\n",
    "\n",
    "#setup the service context (global setting of LLM)\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size = 512\n",
    "\n",
    "#setup the storage context\n",
    "graph_store = SimpleGraphStore()\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "#Construct the Knowlege Graph Undex\n",
    "index = KnowledgeGraphIndex.from_documents( documents=documents,\n",
    "                                           max_triplets_per_chunk=3,\n",
    "                                           storage_context=storage_context,\n",
    "                                           embed_model=embed_model,\n",
    "                                          include_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hazrat Ali is a significant figure in Islam who is known for his contributions to the betterment of Islam and humanity. He was not only an Imam for Shia Muslims, but for all of mankind. He is considered the best human being to have ever lived after Prophet Muhammad. He was a Warrior-Saint of Islam and spent his life fighting holy wars and promoting knowledge. He was also known for his role as a Caliph and Ruler, promising safety, security, and religious freedom to non-Muslims. He was recognized for his sound judgments and advice based on the Holy Quran. Despite facing challenges, he continued to assist the ruling Caliph and worked towards eradicating abuse and corruption from public service. He lived a humble life and treated the treasures of the Commonwealth as the property of the nation. He was also known for his love for his family, particularly his wife Fatima, the daughter of the Holy Prophet.\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Hazrat Ali?\"\n",
    "query_engine = index.as_query_engine(include_text=True,\n",
    "                                     response_mode =\"tree_summarize\",\n",
    "                                     embedding_mode=\"hybrid\",\n",
    "                                     similarity_top_k=5,)\n",
    "#\n",
    "message_template =f\"\"\"<|system|>Please check if the following pieces of context has any mention of the  keywords provided in the Question.If not then don't know the answer, just say that you don't know.Stop there.Please donot try to make up an answer.</s>\n",
    "<|user|>\n",
    "Question: {query}\n",
    "Helpful Answer:\n",
    "</s>\"\"\"\n",
    "#\n",
    "response = query_engine.query(message_template)\n",
    "#\n",
    "print(response.response.split(\"<|assistant|>\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One event that showcases Hazrat Ali's bravery is when he risked his life for Prophet Muhammad during the Prophet's flight to Medina. The Prophet deputed Ali to lie in his bed, knowing that his enemies wanted to kill him. Thus, it was Ali who faced the danger in place of his Master.\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me about the bravery of Hazrat Ali. Give one event from his life that shows his bravery.\"\n",
    "query_engine = index.as_query_engine(include_text=True,\n",
    "                                     response_mode =\"tree_summarize\",\n",
    "                                     embedding_mode=\"hybrid\",\n",
    "                                     similarity_top_k=5,)\n",
    "#\n",
    "message_template =f\"\"\"<|system|>Please check if the following pieces of context has any mention of the  keywords provided in the Question.If not then don't know the answer, just say that you don't know.Stop there.Please donot try to make up an answer.</s>\n",
    "<|user|>\n",
    "Question: {query}\n",
    "Helpful Answer:\n",
    "</s>\"\"\"\n",
    "#\n",
    "response = query_engine.query(message_template)\n",
    "#\n",
    "print(response.response.split(\"<|assistant|>\")[-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "from IPython.display import display, HTML\n",
    "g = index.get_networkx_graph()\n",
    "net = Network(notebook=True,cdn_resources=\"in_line\",directed=True)\n",
    "net.from_nx(g)\n",
    "html = net.generate_html()\n",
    "with open(\"./output/example.html\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context.persist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
