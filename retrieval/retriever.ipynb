{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import Config\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = Config.OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "embed_model = OpenAIEmbedding(model_name=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Capelin\\graph-rag-llama-index\\graph-rag-llama-index\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from graph_rag_store import GraphRAGStore\n",
    "\n",
    "# Note: used to be `Neo4jPGStore`\n",
    "graph_store = GraphRAGStore(\n",
    "    username=\"neo4j\", password=\"admin123\", url=\"bolt://localhost:7687\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.schema import QueryBundle\n",
    "\n",
    "NUM_QUERIES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gen_prompt = \"\"\"You are an AI language model assistant specializing in query expansion. Your task is to generate {num_queries} diverse versions of the given user question. These variations will be used to retrieve relevant documents from a vector database, helping to overcome limitations of distance-based similarity search.\n",
    "\n",
    "Original question: {query}\n",
    "\n",
    "Instructions:\n",
    "1. Create {num_queries} unique variations of the original question.\n",
    "2. Ensure each variation maintains the core intent of the original question.\n",
    "3. Use different phrasings, synonyms, or perspectives for each variation.\n",
    "4. Consider potential context or implications not explicitly stated in the original question.\n",
    "5. Avoid introducing new topics or drastically changing the meaning of the question.\n",
    "\n",
    "Please provide your {num_queries} question variations, each on a new line:\n",
    "\"\"\"\n",
    "\n",
    "query_variations = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries(self, original_query: str) -> list[QueryBundle]:  # noqa: ANN001\n",
    "    global query_variations  # Declare the global list  # noqa: PLW0603\n",
    "    prompt_str = self.query_gen_prompt.format(\n",
    "        num_queries=self.num_queries - 1,\n",
    "        query=original_query,\n",
    "    )\n",
    "    response = self._llm.complete(prompt_str)\n",
    "\n",
    "    # Assume LLM properly put each query on a newline\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    queries = [q.strip() for q in queries if q.strip()]\n",
    "\n",
    "    # Store the generated queries in the global variable\n",
    "    query_variations = queries # this is the only change that I made\n",
    "\n",
    "    if self._verbose:\n",
    "        queries_str = \"\\n\".join(queries)\n",
    "        print(f\"Generated queries:\\n{queries_str}\")\n",
    "\n",
    "    # The LLM often returns more queries than we asked for, so trim the list.\n",
    "    return [QueryBundle(q) for q in query_variations]\n",
    "\n",
    "QueryFusionRetriever._get_queries = get_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever(retriever, top_n: int) -> QueryFusionRetriever:\n",
    "    \"\"\"\n",
    "    Creates and returns a QueryFusionRetriever instance configured with the specified number of top results.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        top_n (int): The number of top results to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        QueryFusionRetriever: An instance of QueryFusionRetriever configured with the specified parameters.\n",
    "\n",
    "    \"\"\"\n",
    "    return QueryFusionRetriever(\n",
    "        [retriever],\n",
    "        similarity_top_k=top_n,\n",
    "        num_queries=NUM_QUERIES,\n",
    "        mode=\"simple\",\n",
    "        use_async=True,\n",
    "        verbose=True,\n",
    "        query_gen_prompt=query_gen_prompt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(question, retriever, top_n) -> tuple[None, Any]:\n",
    "    \"\"\"\n",
    "    Retrieve documents based on a given question.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        question (str): The question to retrieve documents for.\n",
    "        top_n (int): The number of top documents to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        tuple[None, Any]: A tuple containing None and the retrieved documents.\n",
    "\n",
    "    \"\"\"\n",
    "    retriever = get_retriever(retriever, top_n)\n",
    "    docs = retriever.retrieve(question)\n",
    "    return query_variations, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PropertyGraphIndex\n",
    "\n",
    "retriever = PropertyGraphIndex.from_existing(\n",
    "    property_graph_store=graph_store,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    show_progress=True,\n",
    "    include_text=True\n",
    ").as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How does the islamic concept of freedom relate to divine justice and why there is suffering in the world?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. In what ways does the Islamic belief in freedom intersect with the idea of divine justice and the existence of suffering in the world?\n",
      "2. How is the Islamic principle of freedom connected to the concept of divine justice and the presence of suffering in the world?\n",
      "3. What is the relationship between the Islamic notion of freedom, divine justice, and the existence of suffering in the world?\n",
      "4. How does the Islamic perspective on freedom influence the understanding of divine justice and the reason behind suffering in the world?\n"
     ]
    }
   ],
   "source": [
    "query_variations, nodes = retrieve_documents(question=question, retriever=retriever, top_n=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, llama_index.core.schema.NodeWithScore)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes), type(nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7316093444824219"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "George Sarton -> Advocate -> RELIGIOUS FAITH"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "display(Markdown(f\"{nodes[0].text}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# create a dictionary to store the query and nodes data\n",
    "output_data = {\n",
    "    'original_question': question,\n",
    "    'query_variations': query_variations,\n",
    "    'nodes': []\n",
    "}\n",
    "\n",
    "# extract relevant information from each node\n",
    "for node in nodes:\n",
    "    node_info = {\n",
    "        'text': node.node.text,\n",
    "        'score': float(node.score),  # convert to float for JSON serialization\n",
    "        'metadata': node.node.metadata\n",
    "    }\n",
    "    output_data['nodes'].append(node_info)\n",
    "\n",
    "# write to JSON file\n",
    "with open('output/retrieved_nodes.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
